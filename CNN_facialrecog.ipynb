{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import *\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>emotion</td>\n",
       "      <td>pixels</td>\n",
       "      <td>Usage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35883</th>\n",
       "      <td>6</td>\n",
       "      <td>50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>3</td>\n",
       "      <td>178 174 172 173 181 188 191 194 196 199 200 20...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>0</td>\n",
       "      <td>17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>3</td>\n",
       "      <td>30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35887</th>\n",
       "      <td>2</td>\n",
       "      <td>19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35888 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       emotion                                             pixels        usage\n",
       "0      emotion                                             pixels        Usage\n",
       "1            0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...     Training\n",
       "2            0  151 150 147 155 148 133 111 140 170 174 182 15...     Training\n",
       "3            2  231 212 156 164 174 138 161 173 182 200 106 38...     Training\n",
       "4            4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...     Training\n",
       "...        ...                                                ...          ...\n",
       "35883        6  50 36 17 22 23 29 33 39 34 37 37 37 39 43 48 5...  PrivateTest\n",
       "35884        3  178 174 172 173 181 188 191 194 196 199 200 20...  PrivateTest\n",
       "35885        0  17 17 16 23 28 22 19 17 25 26 20 24 31 19 27 9...  PrivateTest\n",
       "35886        3  30 28 28 29 31 30 42 68 79 81 77 67 67 71 63 6...  PrivateTest\n",
       "35887        2  19 13 14 12 13 16 21 33 50 57 71 84 97 108 122...  PrivateTest\n",
       "\n",
       "[35888 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'faces_data.csv'\n",
    "column_names=['emotion','pixels','usage']\n",
    "df=pd.read_csv(file_name,names=column_names, na_filter=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and split columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readlines(file_name):\n",
    "    features = []\n",
    "    target_variable = []\n",
    "    first_column = True\n",
    "\n",
    "    with open(file_name) as file_:\n",
    "        for line in file_:\n",
    "            if first_column:\n",
    "                first_column = False\n",
    "            else:\n",
    "                row = line.split(',')\n",
    "                target_variable.append(int(row[0]))\n",
    "                features.append([int(p) for p in row[1].split()])\n",
    "        features, target_variable = np.array(features) / 255.0, np.array(target_variable)\n",
    "    return features, target_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this dataset, we have  7 classes.\n",
      "(35887, 48, 48, 1)\n",
      "82683648\n",
      "35887\n"
     ]
    }
   ],
   "source": [
    "features, target_variable = readlines(file_name)\n",
    "num_class = len(set(target_variable))\n",
    "print(\"In this dataset, we have \",num_class, \"classes.\")\n",
    "#\n",
    "features = features.reshape(features.shape[0], 48, 48, 1)\n",
    "features.shape\n",
    "#\n",
    "print(features.shape)\n",
    "print(features.size)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape (28709, 7) \n",
      " Test dataset shape (7178, 7)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target_variable, train_size=0.8, random_state=42, shuffle=False)\n",
    "y_train = (np.arange(num_class) == y_train[:, None]).astype(np.float32)\n",
    "y_test = (np.arange(num_class) == y_test[:, None]).astype(np.float32)\n",
    "print(\"Train dataset shape\",y_train.shape,\"\\n\",\"Test dataset shape\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 12, 12, 512)       590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 4,478,727\n",
      "Trainable params: 4,474,759\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialising CNN\n",
    "model = Sequential()\n",
    "# 1 - Convolution\n",
    "model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "# 2nd Convolution layer\n",
    "model.add(Conv2D(128,(5,5), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "# 3rd Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "# 4th Convolution layer\n",
    "model.add(Conv2D(512,(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "# Fully connected layer 1st layer\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "# Fully connected layer 2nd layer\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "449/449 [==============================] - 945s 2s/step - loss: 2.0194 - accuracy: 0.2262 - val_loss: 1.9236 - val_accuracy: 0.2692\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 948s 2s/step - loss: 1.8374 - accuracy: 0.2850 - val_loss: 1.7694 - val_accuracy: 0.3128\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 923s 2s/step - loss: 1.7323 - accuracy: 0.3308 - val_loss: 1.6822 - val_accuracy: 0.3553\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 898s 2s/step - loss: 1.6390 - accuracy: 0.3688 - val_loss: 1.7111 - val_accuracy: 0.3657\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 882s 2s/step - loss: 1.5840 - accuracy: 0.3876 - val_loss: 1.6261 - val_accuracy: 0.3937\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 1196s 3s/step - loss: 1.5202 - accuracy: 0.4159 - val_loss: 1.8263 - val_accuracy: 0.3617\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 899s 2s/step - loss: 1.4598 - accuracy: 0.4365 - val_loss: 1.5574 - val_accuracy: 0.4220\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 889s 2s/step - loss: 1.4179 - accuracy: 0.4564 - val_loss: 1.3536 - val_accuracy: 0.4790\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 877s 2s/step - loss: 1.3690 - accuracy: 0.4711 - val_loss: 1.3351 - val_accuracy: 0.4813\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 905s 2s/step - loss: 1.3316 - accuracy: 0.4921 - val_loss: 1.2773 - val_accuracy: 0.5021\n"
     ]
    }
   ],
   "source": [
    "path_model='model_output.h5'\n",
    "# fit the model\n",
    "h=model.fit(x=X_train,     \n",
    "            y=y_train, \n",
    "            batch_size=64, \n",
    "            epochs=10, \n",
    "            verbose=1, \n",
    "            validation_data=(X_test,y_test),\n",
    "            shuffle=True,\n",
    "            callbacks=[\n",
    "                ModelCheckpoint(filepath=path_model),\n",
    "            ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19986106 0.0181735  0.43980545 ... 0.18841545 0.05887    0.08307365]\n",
      " [0.09861285 0.03665795 0.18158692 ... 0.15000238 0.03675894 0.18423952]\n",
      " [0.197835   0.0324081  0.20266789 ... 0.256772   0.01153912 0.29464996]\n",
      " ...\n",
      " [0.12121066 0.00521638 0.07529508 ... 0.28911346 0.0099457  0.29014105]\n",
      " [0.22287619 0.01299356 0.05565302 ... 0.32807148 0.00733717 0.20502801]\n",
      " [0.16569526 0.0170505  0.21747993 ... 0.46748635 0.00717183 0.10558068]]\n",
      "(7178, 7)\n"
     ]
    }
   ],
   "source": [
    "pred_y=model.predict(X_test)\n",
    "print(pred_y)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model to json file to be used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "model.save_weights('model_weights.h5')\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
